<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>The Constraint Trap: How Deep Research Quietly Cheats</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <style>
body {
  font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
  line-height: 1.65;
  color: #1a1a1a;
  max-width: 760px;
  margin: 40px auto;
  padding: 0 20px;
}

    h1, h2, h3 { line-height: 1.25; }
    h1{
      font-size: clamp(2.0rem, 3.2vw, 2.6rem);
      letter-spacing: -0.02em;
      margin-bottom: 0.5em;
    }

    h2 {
      margin-top: 2.6em;
      border-bottom: 1px solid #eaeaea;
      padding-bottom: 0.3em;
    }

    h3 { margin-top: 1.8em; }

    p { margin: 1.1em 0; }

    a {
      color: #111;
    }

    a:visited {
      color: #111;
    }

    a:hover,
    a:focus {
      color: #000;
    }

    ul { margin: 1em 0 1.2em; padding-left: 1.2em; }
    li { margin: 0.45em 0; }

    table.comparison {
      width: 100%;
      border-collapse: collapse;
      margin: 0;
      font-size: 0.95em;
      line-height: 1.5;
    }

    /* Table styling (kept for potential future use). */
    .table-card{
      margin: 1.6em 0 2.2em;
      border: 1px solid rgba(0,0,0,0.08);
      border-radius: 16px;
      background: #fff;
      overflow: hidden;
      box-shadow: 0 1px 1px rgba(0,0,0,0.04), 0 8px 18px rgba(0,0,0,0.05);
      padding: 12px 12px 10px; /* match compare-card inner padding vibe */
    }

    table.comparison th,
    table.comparison td {
      border-top: 1px solid rgba(0,0,0,0.10);
      padding: 12px 10px;
      vertical-align: top;
      word-break: normal;
      overflow-wrap: break-word;
      hyphens: manual;
    }

    table.comparison thead th {
      border-top: 1px solid rgba(0,0,0,0.14);
      border-bottom: 1px solid rgba(0,0,0,0.14);
      text-align: left;
      font-weight: 650;
      background: transparent;
    }

    .table-brand{
      display: inline-flex;
      align-items: center;
      gap: 8px;
      white-space: nowrap;
      padding: 8px 10px;
      border-radius: 14px;
      border: 1px solid rgba(0,0,0,0.08);
      background: rgba(0,0,0,0.015);
      box-shadow: 0 1px 1px rgba(0,0,0,0.04);
    }

    .table-icon-frame{
      width: 26px;
      height: 26px;
      border-radius: 8px;
      border: 1px solid rgba(0,0,0,0.14);
      background: rgba(0,0,0,0.02);
      display: inline-flex;
      align-items: center;
      justify-content: center;
      overflow: hidden;
      flex: 0 0 auto;
    }
    .table-icon-frame img{
      width: 18px;
      height: 18px;
      display: block;
      object-fit: contain;
    }

    table.comparison tbody tr:nth-child(odd){
      background: rgba(0,0,0,0.015);
    }
    table.comparison tbody tr:hover{
      background: rgba(0,0,0,0.03);
    }
    table.comparison td:first-child{
      color: rgba(0,0,0,0.72);
    }

    table.comparison code {
      font-size: 0.95em;
    }

    @media (max-width: 820px) {
      table.comparison td {
        padding: 10px 8px;
      }
    }

    blockquote {
      border-left: 4px solid #ddd;
      padding-left: 16px;
      color: #555;
      font-style: italic;
      margin: 1.8em 0;
    }

    .divider {
      margin: 3em 0;
      text-align: center;
      color: #aaa;
    }
    .divider.compact {
      margin: 1.6em 0 1.8em;
    }

    .lede {
      font-size: 1.05em;
      color: #222;
    }

    blockquote.callout {
      margin: 2.0em auto;
      padding: 0;
      border-left: 0;
      max-width: 720px;
      text-align: center;
      color: #555;
      font-style: italic;
      font-weight: 450;
      letter-spacing: -0.01em;
      font-size: 1.18em;
      line-height: 1.45;
    }

    blockquote.callout.thesis {
      max-width: 640px;
      font-size: 1.32em;
      font-weight: 600;
      color: #2a2a2a;
    }

    blockquote.callout::before {
      content: "“";
      display: block;
      font-style: normal;
      font-weight: 650;
      font-size: 2.3em;
      line-height: 0.8;
      margin-bottom: 0.15em;
      color: rgba(0, 0, 0, 0.18);
    }

    blockquote.callout::after {
      content: "";
      display: block;
      width: 72px;
      height: 1px;
      background: rgba(0, 0, 0, 0.14);
      margin: 0.9em auto 0;
    }

    @media (max-width: 520px) {
      blockquote.callout {
        font-size: 1.10em;
      }
    }

    .intro-hook{
      font-size: 1.08em;
      line-height: 1.7;
      color: rgba(0,0,0,0.78);
      margin: 0.4em 0 0.9em;
      letter-spacing: -0.01em;
      max-width: none;
      width: 100%;
    }

    p.intro-lede{
      font-size: 1.15em;
      line-height: 1.75;
      color: rgba(0,0,0,0.72);
      margin: 1.1em 0 0;
      max-width: none;
      width: 100%;
    }

    .intro-promise{
      margin: 1.0em 0 1.6em;
      font-size: 1.02em;
      color: rgba(0,0,0,0.68);
    }

    .compare-label{
      margin: 1.2em 0 0.9em;
      font-size: 1.05em;
      font-weight: 650;
      letter-spacing: -0.01em;
      text-transform: none;
      color: rgba(0,0,0,0.75);
    }

    .compare-note{
      margin: -0.4em 0 1.2em;
      font-size: 0.98em;
      line-height: 1.6;
      color: rgba(0,0,0,0.64);
    }
    .compare-note strong{
      font-weight: 650;
      color: rgba(0,0,0,0.72);
    }

    /* Inline attribution logos (tiny, subtle). */
    .inline-logo{
      height: 20px;
      width: auto;
      margin-left: 6px;
      vertical-align: text-bottom;
      opacity: 0.75;
      display: inline-block;
    }

    figure {
      margin: 2.4em 0;
      page-break-inside: avoid;
      break-inside: avoid;
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    figure img {
      width: 100%;
      max-width: 860px;
      height: auto;
      display: block;

      border-radius: 14px;
      border: 1px solid rgba(0, 0, 0, 0.08);
      background: #fff;

      box-shadow:
        0 1px 2px rgba(0, 0, 0, 0.05),
      0 12px 28px rgba(0, 0, 0, 0.08);
    }

    figure video {
      width: 100%;
      max-width: 860px;
      height: auto;
      display: block;
      border-radius: 14px;
      border: 1px solid rgba(0, 0, 0, 0.08);
      background: #fff;
      box-shadow:
        0 1px 2px rgba(0, 0, 0, 0.05),
      0 12px 28px rgba(0, 0, 0, 0.08);
    }

    img.zoomable {
      cursor: zoom-in;
    }

    figure.small img,
    figure.small figcaption {
      max-width: 520px;
    }

    figure.tiny img,
    figure.tiny figcaption {
      max-width: 420px;
    }

    figcaption {
      width: 100%;
      max-width: 860px;

      margin-top: 0.85em;
      color: #666;
      font-size: 0.92em;
      line-height: 1.45;
      text-align: center;
    }

    figcaption code {
      font-size: 0.95em;
    }

    figure.evidence-panels {
      width: 100%;
      max-width: 860px;
      margin: 2.4em auto;
      display: block;
    }

    figure.evidence-panels .grid {
      display: grid;
      grid-template-columns: 1fr;
      gap: 14px;
    }

    figure.evidence-panels .panel {
      border: 1px solid rgba(0,0,0,0.10);
      border-radius: 14px;
      background: #fff;
      overflow: hidden;
      box-shadow: 0 1px 2px rgba(0,0,0,0.05), 0 10px 24px rgba(0,0,0,0.06);
    }

    figure.evidence-panels .label {
      padding: 10px 12px;
      font-size: 0.85em;
      font-weight: 650;
      color: #333;
      border-bottom: 1px solid rgba(0,0,0,0.08);
      background: rgba(0,0,0,0.02);
    }

    figure.evidence-panels img {
      width: 100%;
      max-width: 100%;
      display: block;
      border: 0;
      border-radius: 0;
      box-shadow: none;
    }

    figure.evidence-panels figcaption {
      margin-top: 12px;
      text-align: left;
    }

    @media (min-width: 860px) {
      figure.evidence-panels .grid {
        grid-template-columns: 1fr 1fr;
        align-items: start;
      }
    }

    .img-modal {
      position: fixed;
      inset: 0;
      display: none;
      align-items: center;
      justify-content: center;
      padding: 22px;
      background: rgba(0,0,0,0.72);
      z-index: 9999;
    }

    .img-modal.open {
      display: flex;
    }

    .img-modal-inner {
      max-width: min(1200px, 95vw);
      max-height: 92vh;
    }

    .img-modal img {
      width: 100%;
      height: auto;
      max-height: 92vh;
      object-fit: contain;
      display: block;
      border-radius: 14px;
      box-shadow: 0 18px 60px rgba(0,0,0,0.45);
    }

    .img-modal button {
      position: fixed;
      top: 14px;
      right: 14px;
      width: 44px;
      height: 44px;
      border-radius: 999px;
      border: 1px solid rgba(255,255,255,0.25);
      background: rgba(0,0,0,0.35);
      color: #fff;
      font-size: 22px;
      line-height: 1;
      cursor: pointer;
    }

    body.modal-open {
      overflow: hidden;
    }

    .compare-grid {
      margin: 1.8em 0 2.2em;
      display: grid;
      grid-template-columns: 1fr;
      gap: 14px;
    }
    @media (min-width: 860px) {
      .compare-grid {
        grid-template-columns: 1fr 1fr 1fr;
      }
    }

    /* 2-up variant for secondary comparisons (e.g., retrievers). */
    .compare-grid.two-up{
      grid-template-columns: 1fr;
    }
    @media (min-width: 860px){
      .compare-grid.two-up{
        grid-template-columns: 1fr 1fr;
      }
    }

    .compare-card {
      border: 1px solid rgba(0,0,0,0.08);
      border-radius: 16px;
      background: #fff;
      padding: 16px 16px 14px;
      box-shadow: 0 1px 1px rgba(0,0,0,0.04), 0 8px 18px rgba(0,0,0,0.05);
      display: flex;
      flex-direction: column;
    }
    .compare-card.featured{
      border-top: 3px solid rgba(0,0,0,0.18);
    }

    .compare-title {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 8px;
      margin-bottom: 10px;
    }

    /* Keep KPI sections aligned across cards on desktop by normalizing the header block height. */
    @media (min-width: 860px) {
      .compare-title{
        display: grid;
        /* 3 rows: brand box, summary, chips */
        grid-template-rows: 56px 52px 52px;
        justify-items: center;
        align-items: start;
        gap: 8px;
        margin-bottom: 10px;
      }

      .summary{
        height: 52px;
        overflow: hidden; /* keep headers equal height across cards */
      }

      .chip-row{
        height: 52px;
      }
    }

    .compare-title h3 {
      margin: 0;
      font-size: 1.05em;
    }

    .brand-badge {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      width: 100%;
      box-sizing: border-box;
      padding: 8px 10px;
      border-radius: 14px;
      border: 1px solid rgba(0,0,0,0.08);
      background: rgba(0,0,0,0.015);
      box-shadow: 0 1px 1px rgba(0,0,0,0.04);
      height: 56px; /* keep header boxes identical */
    }

    .brand-row {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      gap: 10px;
      flex-wrap: nowrap; /* keep logo next to name */
      width: 100%;
    }

    .brand-name {
      margin: 0;
      font-size: 1.05em;
      line-height: 1.15;
      text-align: center;
      overflow: hidden;
      display: -webkit-box;
      -webkit-line-clamp: 2;
      -webkit-box-orient: vertical;
    }

    .summary {
      display: flex;
      justify-content: center;
      width: 100%;
      align-items: center;
      text-align: center;
      padding: 6px 0;
      box-sizing: border-box;
      font-size: 0.92em;
      letter-spacing: -0.01em;
      color: rgba(0,0,0,0.62);
    }
    .summary-text {
      font-weight: 600;
      color: rgba(0,0,0,0.70);
      line-height: 1.2;
    }

    .brand-icon-frame {
      width: 28px;
      height: 28px;
      border-radius: 8px;
      border: 1px solid rgba(0,0,0,0.14);
      background: rgba(0,0,0,0.02);
      display: inline-flex;
      align-items: center;
      justify-content: center;
      overflow: hidden;
      flex: 0 0 auto;
    }

    .brand-icon {
      width: 20px;
      height: 20px;
      display: block;
    }

    .brand-placeholder {
      width: 100%;
      height: 100%;
      display: inline-flex;
      align-items: center;
      justify-content: center;
      font-size: 12px;
      font-weight: 750;
      letter-spacing: 0.02em;
      color: rgba(0,0,0,0.65);
    }

    /* Summary uses .summary (not a pill) to avoid competing with the name box. */

    .kpi {
      display: grid;
      gap: 8px;
      margin: 10px 0 0;
      padding: 0;
      list-style: none;
      flex: 1;
    }

    .kpi li {
      margin: 0;
      padding-top: 8px;
      border-top: 1px solid rgba(0,0,0,0.08);
      display: block;
    }
    .kpi li:first-child {
      border-top: 0;
      padding-top: 0;
    }

    .kpi b {
      display: block;
      font-weight: 650;
      color: #222;
      margin-bottom: 1px;
    }

    /* --- Chips (use-case tags) --- */
    .chip-row{
      display: flex;
      gap: 6px;
      flex-wrap: wrap;
      justify-content: center;
      margin-top: 2px;
      margin-bottom: 2px;
    }

    .chip{
      display: inline-flex;
      align-items: center;
      padding: 3px 8px;
      border-radius: 999px;
      font-size: 0.78em;
      line-height: 1.2;
      border: 1px solid rgba(0,0,0,0.10);
      background: rgba(0,0,0,0.02);
      color: rgba(0,0,0,0.68);
      letter-spacing: -0.005em;
      white-space: nowrap;
    }

    .chip.strong{
      border-color: rgba(0,0,0,0.16);
      background: rgba(0,0,0,0.04);
      color: rgba(0,0,0,0.78);
      font-weight: 600;
    }

    .chip.accent{
      border-color: rgba(0,0,0,0.14);
      background: rgba(0,0,0,0.03);
      color: rgba(0,0,0,0.78);
      font-weight: 650;
    }


    .deck{
      margin: 0.3em 0 1.2em;
      color: rgba(0,0,0,0.65);
      font-size: 1.02em;
      line-height: 1.6;
      letter-spacing: -0.01em;
    }

    @media (min-width: 860px){
      /* Height is normalized via the .compare-title desktop grid; keep this empty as a reminder. */
    }
    </style>
  </head>

  <body>

    <h1>The Constraint Trap: How Deep Research Quietly Cheats</h1>

    <p class="intro-lede">
    Deep research systems are everywhere now. They can search the web, synthesize dozens of sources, and produce polished reports in minutes. On the surface, they already do <strong>everything you’d want</strong>:
    filter by date, restrict domains, plug in a time window, add a few trusted sources, and let the
    system do the rest.
    </p>

    <blockquote class="callout">It sounds almost trivial — until you need those constraints to actually hold.</blockquote>

    <p>
      The problem isn’t that these systems are broken. They’re doing what they’re designed to do:
      optimize for coherent, helpful synthesis. But when constraints matter, coherence isn’t the goal.
      <strong>Correctness under hard boundaries is.</strong>
    </p>

    <p>
      We evaluated several existing deep-research approaches and saw a consistent pattern: each is strong,
      but each optimizes for a different failure mode.
    </p>

    <p class="compare-note">
      <strong>Constraint-aware agent:</strong> a purpose-built research pipeline that we built to enforce explicit constraints end-to-end
      and fail closed when they can’t be satisfied.
    </p>

    <section class="compare-grid" aria-label="Comparison: Perplexity vs OpenAI Deep Research vs our constraint-aware agent">
      <div class="compare-card">
        <div class="compare-title">
          <div class="brand-badge">
            <div class="brand-row">
              <span class="brand-icon-frame" aria-hidden="true">
                <img class="brand-icon" src="./perplexity_mark.svg" alt="" />
              </span>
              <h3 class="brand-name"><a href="https://www.perplexity.ai/" target="_blank" rel="noreferrer">Perplexity</a></h3>
            </div>
          </div>
          <div class="summary"><span class="summary-text">Fast, soft constraints</span></div>
          <div class="chip-row" aria-label="Use cases">
            <span class="chip strong">Exploration</span>
            <span class="chip">Breadth</span>
            <span class="chip">Fast answers</span>
          </div>
        </div>
        <ul class="kpi">
          <li><b>Optimizes for</b> Speed and breadth over strict correctness</li>
          <li><b>When to use</b> Quick exploration, early signal scanning, idea generation</li>
          <li><b>Tradeoff</b> Soft constraints under pressure</li>
        </ul>
      </div>

      <div class="compare-card">
        <div class="compare-title">
          <div class="brand-badge">
            <div class="brand-row">
              <span class="brand-icon-frame" aria-hidden="true">
                <img class="brand-icon" src="./openai_mark.svg" alt="" />
              </span>
              <h3 class="brand-name"><a href="https://openai.com/index/introducing-deep-research/" target="_blank" rel="noreferrer">OpenAI Deep Research</a></h3>
            </div>
          </div>
          <div class="summary"><span class="summary-text">Polished, slow cadence</span></div>
          <div class="chip-row" aria-label="Use cases">
            <span class="chip strong">Deep dive</span>
            <span class="chip">Synthesis</span>
            <span class="chip">One-off</span>
          </div>
        </div>
        <ul class="kpi">
          <li><b>Optimizes for</b> Coherent synthesis and readable narratives</li>
          <li><b>When to use</b> One-off deep dives and background briefs</li>
          <li><b>Tradeoff</b> Slow cadence for repeated runs</li>
        </ul>
      </div>

      <div class="compare-card featured">
        <div class="compare-title">
          <div class="brand-badge">
            <div class="brand-row">
              <h3 class="brand-name">Constraint-aware agent</h3>
            </div>
          </div>
          <div class="summary"><span class="summary-text">Hard constraints, fast runs</span></div>
          <div class="chip-row" aria-label="Use cases">
            <span class="chip accent">Recurring</span>
            <span class="chip">Hard constraints</span>
            <span class="chip">Fast cadence</span>
          </div>
        </div>
        <ul class="kpi">
          <li><b>Optimizes for</b> Hard domain and time guarantees, even at the cost of empty results</li>
          <li><b>When to use</b> Recurring reports and constraint-sensitive research</li>
          <li><b>Tradeoff</b> Fails closed when constraints can’t be satisfied</li>
        </ul>
      </div>
    </section>

    <p>
      The failures we cared about weren’t subtle. They were operational:
      you ask for a strict time window and get out-of-window sources;
      you whitelist domains and unapproved sources slip in;
      citations look fine until you verify them.
    </p>

    <p>
      These violations don’t show up as errors. They show up as <strong>quiet drift</strong> —
      wrapped in otherwise coherent output. That’s what makes them risky in practice.
    </p>

    <figure>
      <img class="zoomable" tabindex="0" src="./tavily-failure.png"
      alt="Perplexity returning sources outside date and domain constraints">
      <figcaption>
        Perplexity example: explicit date window and domain whitelist — yet out-of-bounds sources still appear in the result.
      </figcaption>
    </figure>

    <p>
      We ran the same constraint-heavy requests through OpenAI Deep Research. The report quality was strong,
      but the runtime was too slow to be operational — often <strong>20+ minutes</strong>.
      In contrast, our constraint-aware agent completes comparable runs in about <strong>3 minutes</strong>,
      making recurring and iterative use feasible.
    </p>

    <p class="lede">
      Deep research agents promise to scan the landscape, filter the noise, and surface what matters — repeatedly, and within clear boundaries.
      In practice, <strong>those boundaries are the most fragile part</strong> of the system.
    </p>

    <div class="divider compact">—</div>

    <p>
      We built this agent for a narrow role: act as a <strong>trend scout</strong> for internal research groups,
      generating recurring reports within a fixed historical range, from a trusted set of sources,
      in a standardized format — fast enough to run regularly.
    </p>

    <figure class="tiny">
      <video controls playsinline preload="metadata">
        <source src="./Openai%20DR.mp4" type="video/mp4">
      </video>
      <figcaption>OpenAI Deep Research run (often 20+ minutes).</figcaption>
    </figure>

    <figure class="tiny">
      <video controls playsinline preload="metadata">
        <source src="./Demo%202.mp4" type="video/mp4">
      </video>
      <figcaption>Constraint-aware agent run (about 3 minutes).</figcaption>
    </figure>

    <h2>Making deep research dependable</h2>

    <p>
      On paper, the problem looked straightforward. Search engines support date filters. Research tools allow domain restrictions.
      The hard part wasn’t specifying constraints — it was making them <strong>survive the whole pipeline</strong>:
      retrieval, metadata, fallbacks, and synthesis.
    </p>

    <p>
      We inherited an early prototype that worked for demos, but its behavior was hard to reason about and hard to tighten.
      We rebuilt on top of <strong><a href="https://github.com/assafelovic/gpt-researcher" target="_blank" rel="noreferrer">GPT Researcher</a></strong>
      not to make the agent “smarter,” but to start from a clean, understandable baseline we could extend systematically.
    </p>

    <p>
      The key shift was conceptual rather than technical:
      constraints stopped being something the model should “try” to respect and became <strong>product requirements</strong> enforced in code.
      Domain boundaries became explicit whitelists, time windows became hard start/end checks,
      and report structure became a programmatic contract rather than prompted prose.
    </p>

    <blockquote class="callout thesis">
      <strong>Constraints aren’t “preferences.” They’re requirements.</strong>
    </blockquote>

    <p>
      The surprising part is how rarely constraint failures announce themselves.
      They don’t crash the system. They produce outputs that look correct.
      The failures only became obvious once we ran the agent repeatedly — across different domains, time windows, and source distributions —
      and validated results systematically.
    </p>

    <figure class="small">
      <img class="zoomable" tabindex="0" src="./agent-workflow.png"
      alt="High-level research agent workflow">
      <figcaption>
        High-level workflow: planning, parallel retrieval, constraint enforcement,
        and structured synthesis.
      </figcaption>
    </figure>

    <h2>Where things started to fall apart</h2>

    <h3>1. When the agent invents continuity under constraint pressure</h3>

    <p>
    One of the first signals showed up in citations.
    Some reports included references that appeared entirely legitimate:
    plausible titles, publication dates, and summaries that were clearly relevant to the topic.
    At a glance, everything checked out.
    </p>

    <p>
    On closer inspection, it didn’t.
    Publication dates fell outside the selected time window,
    and URLs were often wrong or slightly corrupted.
    In many cases, the referenced article actually existed —
    but the cited link didn’t.
    You could usually find it by manually searching for the title.
    </p>

    <p>
    Understanding why this was happening took time.
    The first instinct was to inspect retrieval,
    which turned out to be behaving correctly.
    What we eventually uncovered was a different failure mode:
    when the system encountered <strong>insufficient valid sources</strong>
    that satisfied all constraints, it didn’t fail or return an empty result.
    It filled the gap itself.
    </p>

    <p>
    Under constraint pressure, the agent produced internally consistent-looking citations
    that violated both temporal and domain boundaries.
    Realizing that this behavior was caused by
    <strong>model hallucination under constraint pressure</strong>
    took significantly longer than we expected.
    </p>

    <p>
    From that point on, “no valid sources found” became a <strong>first-class outcome</strong>,
    not an error to hide or work around.
    </p>

    <h3>2. Retrieval looks solved — until constraints interact</h3>

    <p>
    Even after addressing hallucination under constraint pressure, another fragility remained.
    Most deep research pipelines live or die by retrieval, and this is where constraint enforcement
    proved far more subtle than expected. On paper, retrieval looks solved: filter by domain,
    filter by date, rank by relevance. In practice, it’s the interaction between those filters
    that causes problems to emerge.
    </p>

    <p>
    We initially built the system on top of <strong><a href="https://www.tavily.com/" target="_blank" rel="noreferrer">Tavily</a></strong>.
    With standard settings and basic search depth, Tavily behaved predictably:
    when no sources matched the constraints, it often returned <strong>no results</strong>.
    Compared to other retrievers we tested — particularly those used in systems like Perplexity —
    Tavily discarded a much larger share of irrelevant content.
    The tradeoff was clear: higher precision at the cost of lower recall.
    </p>

    <p>
    The problematic behavior only appeared once we pushed the system harder.
    In advanced search depth, and specifically when <em>no valid sources existed</em>
    for the specified domain and time window, Tavily sometimes returned semantically relevant
    content that violated domain or temporal constraints.
    Faced with an empty result set, the system preferred something plausible over nothing correct.
    </p>

    <p>
    As we expanded the scope, we switched to <strong><a href="https://exa.ai/" target="_blank" rel="noreferrer">Exa</a></strong> to increase coverage.
    The system returned more sources, but that increase in recall came with a familiar cost:
    a larger share of the results was less relevant.
    </p>

    <p>
    While validating historical runs, we noticed that some sources returned by Exa
    were violating the specified time constraints.
    At first, we assumed this was a fault in our own pipeline.
    We inspected our filtering logic and then turned to the retriever metadata
    to understand what was happening.
    </p>

    <p>
    That’s when the issue became visible.
    The <code>publication_date</code> metadata returned by Exa was often missing,
    incomplete, or clearly inaccurate, making the observed filtering behavior
    difficult to explain.
    </p>


    <p>
    Only after inspecting example outputs in Exa’s own documentation did it become clear
    that this behavior wasn’t specific to our implementation.
    The same inconsistencies were visible there as well —
    yet the issue is not explicitly documented or called out as a limitation.
    </p>

    <p>
    More importantly, retriever-side date filtering could not be reliably inferred
    from the returned metadata alone.
    Some sources were filtered out even though their metadata dates appeared to fall
    within the specified time window, while others passed through despite dates being
    clearly wrong.
    </p>

    <p class="compare-label">Retriever tradeoffs</p>
    <section class="compare-grid two-up" aria-label="Comparison: Tavily vs Exa">
      <div class="compare-card">
        <div class="compare-title">
          <div class="brand-badge">
            <div class="brand-row">
              <span class="brand-icon-frame" aria-hidden="true">
                <img class="brand-icon" src="./tavily-color.svg" alt="" />
              </span>
              <h3 class="brand-name"><a href="https://www.tavily.com/" target="_blank" rel="noreferrer">Tavily</a></h3>
            </div>
          </div>
          <div class="summary"><span class="summary-text">Higher precision, lower recall</span></div>
          <div class="chip-row" aria-label="Use cases">
            <span class="chip strong">Precision</span>
            <span class="chip">Accept empty</span>
            <span class="chip">Lower noise</span>
          </div>
        </div>
        <ul class="kpi">
          <li><b>Tradeoff</b> Higher precision, lower recall (may miss coverage)</li>
          <li><b>Behavior under empty constraints</b> At advanced depth, may return “plausible” results that violate domain or time</li>
          <li><b>Date signal quality</b> Not the primary failure mode; issues emerge under fallback behavior</li>
          <li><b>Best used for</b> Precision-focused pipelines where empty results are acceptable</li>
        </ul>
      </div>

      <div class="compare-card">
        <div class="compare-title">
          <div class="brand-badge">
            <div class="brand-row">
              <span class="brand-icon-frame" aria-hidden="true">
                <img class="brand-icon" src="./exa_mark.svg" alt="" />
              </span>
              <h3 class="brand-name"><a href="https://exa.ai/" target="_blank" rel="noreferrer">Exa</a></h3>
            </div>
          </div>
          <div class="summary"><span class="summary-text">Higher recall, more noise</span></div>
          <div class="chip-row" aria-label="Use cases">
            <span class="chip strong">Coverage</span>
            <span class="chip">Post-filter</span>
            <span class="chip">Validation</span>
          </div>
        </div>
        <ul class="kpi">
          <li><b>Tradeoff</b> Higher recall and coverage, lower precision (more noise)</li>
          <li><b>Behavior under empty constraints</b> May surface results that appear in-window but are difficult to validate reliably</li>
          <li><b>Date signal quality</b> <code>publication_date</code> often missing, incomplete, or inaccurate; metadata alone is unreliable</li>
          <li><b>Best used for</b> Broad discovery, followed by strong post-filtering and validation</li>
        </ul>
      </div>
    </section>

    <h3>3. Filtering for correctness isn’t enough — usefulness matters too</h3>

    <p>
    That limitation pushed us to experiment with an additional guardrail.
    We integrated <img class="inline-logo" src="./scrapegraph_mark.svg" alt="" aria-hidden="true" onerror="this.remove()" /><strong><a href="https://scrapegraphai.com/" target="_blank" rel="noreferrer">Scrapegraph AI</a></strong> into the pipeline as a way to extract
    and reason over page content directly, rather than relying solely on retriever
    metadata. The initial motivation was <strong>temporal validation</strong>: confirming publication
    dates from the page itself when metadata could not be trusted.
    </p>

    <p>
    Once in place, it became clear that the same mechanism could be applied more
    broadly. Beyond time filtering, we also faced a <em>criteria filtering</em>
    problem: there are sources that technically fall within the right domain and
    time window, but that are <strong>structurally not useful</strong> — content hidden behind
    paywalls, announcements without code or pricing, or articles that look relevant
    but cannot be acted upon.
    </p>

    <p>
    Using Scrapegraph AI, we experimented with applying natural-language criteria directly
    to extracted page content before final inclusion. In practice, this worked
    surprisingly well as an additional filtering layer on top of retrieval and
    domain constraints.
    </p>

    <p>
    The approach was promising, but it also introduced additional complexity and
    variability. For the MVP, we intentionally <strong>disabled this layer</strong> to keep the system
    predictable and easier to reason about. It will likely return as part of a more
    unified filtering layer that combines temporal validation, quality checks, and
    actionability criteria.
    </p>

    <div class="divider">—</div>

    <p>
    A related but more structural issue emerged once we started comparing outputs
    across systems. When academic repositories such as arXiv were included, they
    tended to dominate the final reports. We observed the same skew in our system,
    OpenAI Deep Research, and Perplexity under comparable scopes.
    </p>

    <p>
    We don’t believe this dominance automatically indicates bias.
    A more plausible explanation is content reality:
    arXiv simply publishes vastly more material than most industry, product, or
    policy-focused sources. In many cases, this is genuinely where new ideas first
    appear.
    </p>

    <p>
    The real question is not whether this skew should be “fixed”, but whether it
    aligns with the intended use case. For enterprise trend scouting, relevance
    alone is often insufficient. <strong>Coverage balance</strong> becomes an explicit design
    decision.
    </p>

    <p>
      For our MVP, we made coverage balance an explicit part of retrieval.
      We explored several strategies to prevent a single domain from dominating the results,
      including reordering combined result lists by domain or splitting retrieval budgets up front.
      In practice, those approaches either added complexity or proved brittle under edge cases.
    </p>

    <p>
      Instead, we implemented a simple retry-based mechanism:
      when a single domain began to dominate the results,
      the system detected that skew and re-ran retrieval without the dominant source.
      This kept coverage balanced while remaining predictable and easy to reason about.
    </p>

    <p>
    Dominant sources can still surface —
    but only if they earn their place rather than overwhelming the signal
    by sheer volume.
    </p>

    <figure class="evidence-panels">
      <div class="grid">
        <div class="panel">
          <div class="label">Query</div>
          <img class="zoomable" tabindex="0" src="./tavily%20bug%201.png" alt="Tavily query and constraints" />
        </div>
        <div class="panel">
          <div class="label">Response</div>
          <img class="zoomable" tabindex="0" src="./tavily%20bug%205.png" alt="Tavily response showing domain dominance and violations" />
        </div>
      </div>
      <figcaption>
        Retrieval output showing domain dominance: although multiple domains were allowed, results collapsed to a single source.
        If you inspect the publication dates, you can also see violations of the specified time window conditions in Tavily’s advanced search depth mentioned before.
      </figcaption>
    </figure>


    <h2>The decisions that quietly made the system reliable</h2>

    <p>
    One of the fastest and most impactful quality improvements had nothing to do with models.
    We removed the free-text input field.
    Instead, users select a research group from a dropdown,
    and the system injects the domain description and constraints automatically.
    </p>

    <p>
    This dramatically reduced input variance and improved consistency.
    Later, we reintroduced limited free text —
    but only for editing the research group title and description,
    not for redefining the task itself.
    </p>

    <div class="divider">—</div>

    <p>
    Several structural decisions proved just as important.
    Early versions relied on prompting the model to follow a template.
    The results looked good — until they didn’t.
    Small deviations accumulated, formatting drifted,
    and reports became harder to compare and validate over time.
    We replaced prompt-level formatting with structured LLM output,
    schema enforcement in code, and deterministic
    JSON → Markdown conversion.
    This removed an entire class of formatting errors
    and made reports easier to render, validate, and evolve.
    Streaming became harder,
    but for this use case reliability mattered more than immediacy.
    </p>

    <p>
    To understand how and why the system behaved the way it did,
    we instrumented the agent using <strong><a href="https://www.langchain.com/langsmith/observability" target="_blank" rel="noreferrer">LangSmith</a></strong>.
    Tracing full runs, inspecting retrieval decisions and intermediate steps,
    and comparing behavior across configurations allowed us to identify
    failure modes that would have been invisible otherwise.
    The goal wasn’t just debugging, but learning —
    feeding those insights back into better defaults,
    stronger guardrails, and more predictable behavior over time.
    </p>

    <p>
    Finally, we deliberately avoided optimizing for a single language model too early.
    Throughout development, we experimented with multiple models,
    treating the model as a replaceable component
    inside a constrained system rather than a fixed choice.
    This helped us understand how hallucination behavior,
    cost, latency, and reasoning depth change under strict constraints —
    and reinforced the idea that architecture and guardrails
    matter more than any single model choice.
    </p>

    <h2>Results so far</h2>

    <p>
      Under the same domain- and time-constrained requests, systems diverged in ways that matter operationally.
      Perplexity and OpenAI Deep Research can produce polished synthesis, but neither treats constraints as hard requirements —
      and violations tend to slip in quietly.
    </p>

    <p>
      This agent behaves differently. Constraints are enforced end-to-end.
      Time windows and domain whitelists hold throughout the pipeline, report structure is deterministic,
      and <strong>“no valid sources found”</strong> is an intentional outcome rather than something to smooth over.
    </p>

    <p>
      Latency reinforced the distinction. In our demo, a full run completes in about <strong>3 minutes</strong>.
      OpenAI Deep Research often took <strong>20+ minutes</strong>, and some runs failed to complete.
      For recurring reporting, that difference determines whether the system is usable at all.
    </p>

    <p>
      The most important shift was qualitative: what began as a promising prototype now behaves like infrastructure —
      something that can be run repeatedly, reasoned about, and improved incrementally without surprising its users.
    </p>

    <h2>What’s next</h2>

    <ul>
      <li>Reintroduce a unified <strong>LLM-as-a-judge</strong> layer for quality and actionability</li>
      <li>Strengthen temporal validation beyond retriever metadata</li>
      <li>Expand curated domains for product, legal, and policy coverage</li>
      <li>Use observability data to systematically optimize the system</li>
    </ul>

    <h2>Closing thought</h2>

    <blockquote class="callout">
      Deep research agents don’t fail because they lack constraints.<br/>
      They fail because <strong>constraint enforcement breaks at the system boundaries</strong> —
      retrieval, metadata, and synthesis.
    </blockquote>

    <p>
      Once you recognize this, the focus shifts. The problem is no longer about crafting better prompts
      or picking a more capable model. It becomes an engineering problem: how constraints are represented,
      propagated, and enforced across the system.
    </p>

    <p>
      When deep research starts behaving like infrastructure — predictable, inspectable, and dependable —
      that’s when it becomes genuinely useful.
    </p>

    <div class="img-modal" id="img-modal" aria-hidden="true">
      <button type="button" id="img-modal-close" aria-label="Close">×</button>
      <div class="img-modal-inner">
        <img id="img-modal-img" alt="" />
      </div>
    </div>
    <script>
      (() => {
        const modal = document.getElementById('img-modal');
        const modalImg = document.getElementById('img-modal-img');
        const closeBtn = document.getElementById('img-modal-close');
        if (!modal || !modalImg || !closeBtn) return;

        const close = () => {
          modal.classList.remove('open');
          modal.setAttribute('aria-hidden', 'true');
          document.body.classList.remove('modal-open');
          modalImg.removeAttribute('src');
        };

        const open = (src, alt) => {
          modalImg.src = src;
          modalImg.alt = alt || '';
          modal.classList.add('open');
          modal.setAttribute('aria-hidden', 'false');
          document.body.classList.add('modal-open');
        };

        document.addEventListener('click', (e) => {
          const t = e.target;
          if (t && t.classList && t.classList.contains('zoomable') && t.tagName === 'IMG') {
            open(t.currentSrc || t.src, t.alt);
          }
        });

        closeBtn.addEventListener('click', close);
        modal.addEventListener('click', (e) => {
          if (e.target === modal) close();
        });
        document.addEventListener('keydown', (e) => {
          const t = e.target;
          if (e.key === 'Enter' && t && t.classList && t.classList.contains('zoomable') && t.tagName === 'IMG') {
            open(t.currentSrc || t.src, t.alt);
          }
          if (e.key === 'Escape') close();
        });
      })();
    </script>
  </body>
</html>
